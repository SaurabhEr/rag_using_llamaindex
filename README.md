# rag_using_llamaindex
# This code uses the BAAI/bge-m3 embedding model for generating embeddings
# and the DeepSeek LLM model (deepseek-r1:7b) for generating answers based on queries.
# The implementation works well for queries related to levels 2 to 4, providing relevant responses.
# However, for level 3-specific queries, it successfully retrieves and generates answers.
